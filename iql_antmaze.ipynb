{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text IQL d4rl antmaze-meduium-play dataset"
   ],
   "metadata": {
    "id": "-CqmegxiQD0-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 1. install dependences for d4rl"
   ],
   "metadata": {
    "id": "ll0ymBQeQcPt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YscyaE1YbNzY",
    "outputId": "2116ddd3-7fa4-4014-fa72-f634614492dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "software-properties-common is already the newest version (0.99.22.9).\n",
      "The following additional packages will be installed:\n",
      "  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
      "  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
      "The following NEW packages will be installed:\n",
      "  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n",
      "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
      "  libosmesa6-dev\n",
      "0 upgraded, 15 newly installed, 0 to remove and 45 not upgraded.\n",
      "Need to get 4,020 kB of archives.\n",
      "After this operation, 19.4 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [6,842 B]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.2 [3,121 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [8,984 B]\n",
      "Fetched 4,020 kB in 2s (2,365 kB/s)\n",
      "Selecting previously unselected package libglx-dev:amd64.\n",
      "(Reading database ... 123594 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl-dev:amd64.\n",
      "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libegl-dev:amd64.\n",
      "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
      "Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package libgles1:amd64.\n",
      "Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgles-dev:amd64.\n",
      "Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
      "Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libopengl-dev:amd64.\n",
      "Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libglvnd-dev:amd64.\n",
      "Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
      "Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Selecting previously unselected package libglu1-mesa:amd64.\n",
      "Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
      "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
      "Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
      "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
      "Selecting previously unselected package libglew-dev:amd64.\n",
      "Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n",
      "Unpacking libglew-dev:amd64 (2.2.0-4) ...\n",
      "Selecting previously unselected package libosmesa6:amd64.\n",
      "Preparing to unpack .../13-libosmesa6_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
      "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Selecting previously unselected package libosmesa6-dev:amd64.\n",
      "Preparing to unpack .../14-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
      "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libgles1:amd64 (1.4.0-1) ...\n",
      "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
      "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
      "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglew-dev:amd64 (2.2.0-4) ...\n",
      "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  patchelf\n",
      "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
      "Need to get 72.1 kB of archives.\n",
      "After this operation, 186 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
      "Fetched 72.1 kB in 1s (110 kB/s)\n",
      "Selecting previously unselected package patchelf.\n",
      "(Reading database ... 123734 files and directories currently installed.)\n",
      "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n",
      "Unpacking patchelf (0.14.3-1) ...\n",
      "Setting up patchelf (0.14.3-1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Collecting free-mujoco-py\n",
      "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl.metadata (586 bytes)\n",
      "Collecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n",
      "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.17.0)\n",
      "Collecting fasteners==0.15 (from free-mujoco-py)\n",
      "  Downloading fasteners-0.15-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n",
      "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.34.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.26.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n",
      "Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.4.0)\n",
      "Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m57.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
      "Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m55.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m203.7/203.7 kB\u001B[0m \u001B[31m14.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 3.0.11\n",
      "    Uninstalling Cython-3.0.11:\n",
      "      Successfully uninstalled Cython-3.0.11\n",
      "Successfully installed Cython-0.29.37 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y \\\n",
    "   libgl1-mesa-dev \\\n",
    "   libgl1-mesa-glx \\\n",
    "   libglew-dev \\\n",
    "   libosmesa6-dev \\\n",
    "   software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf\n",
    "\n",
    "!pip install free-mujoco-py"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 2. install wandb\n"
   ],
   "metadata": {
    "id": "dmjdefCaQtCe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wandb --upgrade"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4U4cr0jnf4_Q",
    "outputId": "6354aec7-88c8-4a39-ebcd-5aa5f202b9b3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.1/7.1 MB\u001B[0m \u001B[31m51.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.3/207.3 kB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.8/301.8 kB\u001B[0m \u001B[31m19.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.12.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 3. login to wandb (you must have wadnb account)"
   ],
   "metadata": {
    "id": "RHrizoruQ6Q0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wandb login --relogin"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wbl0gg_2f5Bw",
    "outputId": "af425e5a-181f-48cf-bb46-e04f0a2ab5b7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 4. install d4rl with dataset for offline RL task"
   ],
   "metadata": {
    "id": "JmjxpJIJQ3rY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmVyd_kxf_ib",
    "outputId": "bb25d2b9-f8c6-4eea-cd3c-898bb17c4798"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting d4rl\n",
      "  Cloning https://github.com/Farama-Foundation/d4rl (to revision master) to /tmp/pip-install-s2jenfck/d4rl_956b0a2691734a2c8f8c8a8158b5c2ad\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/d4rl /tmp/pip-install-s2jenfck/d4rl_956b0a2691734a2c8f8c8a8158b5c2ad\n",
      "  Resolved https://github.com/Farama-Foundation/d4rl to commit 71a9549f2091accff93eeff68f1f3ab2c0e0a288\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting mjrl@ git+https://github.com/aravindr93/mjrl@master#egg=mjrl (from d4rl)\n",
      "  Cloning https://github.com/aravindr93/mjrl (to revision master) to /tmp/pip-install-s2jenfck/mjrl_42bc40af5f96445087c115df180548b6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aravindr93/mjrl /tmp/pip-install-s2jenfck/mjrl_42bc40af5f96445087c115df180548b6\n",
      "  Resolved https://github.com/aravindr93/mjrl to commit 3871d93763d3b49c4741e6daeaebbc605fe140dc\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: gym<0.24.0 in /usr/local/lib/python3.10/dist-packages (from d4rl) (0.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from d4rl) (1.23.1)\n",
      "Requirement already satisfied: mujoco_py in /usr/local/lib/python3.10/dist-packages (from d4rl) (2.1.2.14)\n",
      "Requirement already satisfied: pybullet in /usr/local/lib/python3.10/dist-packages (from d4rl) (3.2.6)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from d4rl) (3.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from d4rl) (2.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from d4rl) (8.1.7)\n",
      "Requirement already satisfied: dm_control>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from d4rl) (1.0.22)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.4.0)\n",
      "Requirement already satisfied: dm-env in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.6)\n",
      "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (0.1.8)\n",
      "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.12.0)\n",
      "Requirement already satisfied: labmaze in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.0.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (4.9.4)\n",
      "Requirement already satisfied: mujoco>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.2.2)\n",
      "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.20.3)\n",
      "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.1.7)\n",
      "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (2.32.3)\n",
      "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (71.0.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (4.64.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0->d4rl) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0->d4rl) (0.0.8)\n",
      "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (0.29.37)\n",
      "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (2.34.2)\n",
      "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (1.17.0)\n",
      "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (0.15)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco_py->d4rl) (2.22)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco_py->d4rl) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco_py->d4rl) (1.6)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco_py->d4rl) (9.4.0)\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=3.2.1->dm_control>=1.0.3->d4rl) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (2024.7.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.1->dm_control>=1.0.3->d4rl) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.1->dm_control>=1.0.3->d4rl) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.1->dm_control>=1.0.3->d4rl) (4.12.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.1->dm_control>=1.0.3->d4rl) (3.19.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "step5. install additional dependences for mojuco module. This step is must have for AntMaze task."
   ],
   "metadata": {
    "id": "xFhCBNqFRLqa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "o6IMbxE6RCuv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "  # Get the prereqs\n",
    "  !apt-get -qq update\n",
    "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "  # Get Mujoco\n",
    "  !mkdir ~/.mujoco\n",
    "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "  !rm mujoco.tar.gz\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc\n",
    "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc\n",
    "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "  !ldconfig\n",
    "  # Install Mujoco-py\n",
    "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "  # run once\n",
    "  !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "  if _mujoco_run_once:\n",
    "    pass\n",
    "except NameError:\n",
    "  _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  try:\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "  except KeyError:\n",
    "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "  try:\n",
    "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  except KeyError:\n",
    "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  # presetup so we don't see output on first env initialization\n",
    "  import mujoco_py\n",
    "  _mujoco_run_once = True\n",
    "#sources of this code block : https://gist.github.com/BuildingAtom/3119ac9c595324c8001a7454f23bf8c8,\n",
    "#https://www.kaggle.com/code/mmdalix/openai-gym-mujoco-env-setup-and-training-2022/notebook"
   ],
   "metadata": {
    "id": "Qwotnrfk-V8k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 6. Install other packages required for IQL"
   ],
   "metadata": {
    "id": "ysIn7JYeRkck"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tqdm==4.64.0\n",
    "# !pip install wandb==0.12.21\n",
    "# !pip install mujoco-py==2.1.2.14\n",
    "!pip install numpy==1.23.1\n",
    "!pip install gym[mujoco_py,classic_control]==0.23.0\n",
    "# !pip install --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install torch#!pip install torch==1.11.0+cu113\n",
    "!pip install pyrallis==0.3.1\n",
    "# !pip install --find-links https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# jax==0.4.1\n",
    "# jaxlib[cuda11_cudnn82]==0.4.1\n",
    "# flax==0.6.1\n",
    "# optax==0.1.3\n",
    "# distrax==0.1.2\n",
    "# chex==0.1.5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qS3Zpdh0g7CO",
    "outputId": "db420415-fcd8-4bae-985f-56242fbd0bf9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tqdm==4.64.0\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl.metadata (57 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/57.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.3/57.3 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.4/78.4 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.64.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed tqdm-4.64.0\n",
      "Collecting numpy==1.23.1\n",
      "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.0/17.0 MB\u001B[0m \u001B[31m48.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
      "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n",
      "albumentations 1.4.13 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n",
      "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n",
      "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.64.0 which is incompatible.\n",
      "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.1 which is incompatible.\n",
      "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed numpy-1.23.1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       },
       "id": "c6c72ff9f61f41da98823e793498e878"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gym==0.23.0 (from gym[classic_control,mujoco_py]==0.23.0)\n",
      "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/624.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m614.4/624.4 kB\u001B[0m \u001B[31m19.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m624.4/624.4 kB\u001B[0m \u001B[31m13.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.23.0->gym[classic_control,mujoco_py]==0.23.0) (1.23.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.23.0->gym[classic_control,mujoco_py]==0.23.0) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.23.0->gym[classic_control,mujoco_py]==0.23.0) (0.0.8)\n",
      "\u001B[33mWARNING: gym 0.23.0 does not provide the extra 'mujoco-py'\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting pygame==2.1.0 (from gym[classic_control,mujoco_py]==0.23.0)\n",
      "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m85.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697631 sha256=52ad347fbc1d2295e7e6758f5bd6fa699d523a3bc6672999bcded2b6eab7f09e\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
      "Successfully built gym\n",
      "Installing collected packages: pygame, gym\n",
      "  Attempting uninstall: pygame\n",
      "    Found existing installation: pygame 2.6.0\n",
      "    Uninstalling pygame-2.6.0:\n",
      "      Successfully uninstalled pygame-2.6.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.23.1\n",
      "    Uninstalling gym-0.23.1:\n",
      "      Successfully uninstalled gym-0.23.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.64.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed gym-0.23.0 pygame-2.1.0\n",
      "\u001B[31mERROR: Invalid requirement: 'torch#!pip': Expected end or semicolon (after name and no valid version specifier)\n",
      "    torch#!pip\n",
      "         ^\u001B[0m\u001B[31m\n",
      "\u001B[0mCollecting pyrallis==0.3.1\n",
      "  Downloading pyrallis-0.3.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting typing-inspect (from pyrallis==0.3.1)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyrallis==0.3.1) (6.0.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyrallis==0.3.1)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyrallis==0.3.1) (4.12.2)\n",
      "Downloading pyrallis-0.3.1-py3-none-any.whl (33 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, typing-inspect, pyrallis\n",
      "Successfully installed mypy-extensions-1.0.0 pyrallis-0.3.1 typing-inspect-0.9.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 7. Connect to Google drive to save/load pytorch models. check that IQL_report folder exists."
   ],
   "metadata": {
    "id": "czAQ4q-URqBX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"/content/drive/\")\n",
    "model_path=f\"MyDrive/IQL_report\"\n",
    "if not os.path.exists(model_path):\n",
    "  os.mkdir(model_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2YEMQaUg7Gp",
    "outputId": "330a4a2e-c84a-4961-c86a-b30e58773436"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Main part with IQL CORL algorithm with parameters and full set up for AntMaze.\n",
    "this cell should start training and save models each 5k steps, after you can choose 1 best and use it for evaluation. In order to skip training process you can skip this cell, put pytorch model (checkpoint_194999.pt) from project for this adress (in the folder created in previous step/cell) (to this adress: /content/drive/MyDrive/IQL_report/checkpoint_194999.pt) and then you can run evoluation."
   ],
   "metadata": {
    "id": "_WTGaGadR8j1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# source: https://github.com/gwthomas/IQL-PyTorch\n",
    "# https://arxiv.org/pdf/2110.06169.pdf\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import d4rl\n",
    "import gym\n",
    "import numpy as np\n",
    "import pyrallis\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.distributions import Normal\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "TensorBatch = List[torch.Tensor]\n",
    "\n",
    "\n",
    "EXP_ADV_MAX = 100.0\n",
    "LOG_STD_MIN = -20.0\n",
    "LOG_STD_MAX = 2.0\n",
    "\n",
    "#actor_lr: 3e-4\n",
    "# batch_size: 256\n",
    "# beta: 10.0\n",
    "# buffer_size: 10000000\n",
    "# checkpoints_path: null\n",
    "# device: cuda\n",
    "# discount: 0.99\n",
    "# env: antmaze-medium-play-v2\n",
    "# eval_freq: 5000\n",
    "# group: iql-antmaze-medium-play-v2-multiseed-v0\n",
    "# iql_deterministic: false\n",
    "# iql_tau: 0.9\n",
    "# load_model: ''\n",
    "# max_timesteps: 1000000\n",
    "# n_episodes: 100\n",
    "# name: IQL\n",
    "# normalize: true\n",
    "# normalize_reward: true\n",
    "# qf_lr: 3e-4\n",
    "# project: CORL\n",
    "# seed: 0\n",
    "# tau: 0.005\n",
    "# vf_lr: 3e-4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # wandb project name\n",
    "    project: str = \"CORL\"\n",
    "    # wandb group name\n",
    "    group: str = \"IQL-D4RL\"\n",
    "    # wandb run name\n",
    "    name: str = \"IQL\"\n",
    "    # training dataset and evaluation environment\n",
    "    env: str = \"antmaze-medium-play\"#\"antmaze-medium-diverse\"#\"halfcheetah-medium-expert-v2\"\n",
    "    # discount factor\n",
    "    discount: float = 0.99\n",
    "    # coefficient for the target critic Polyak's update\n",
    "    tau: float = 0.005\n",
    "    # actor update inverse temperature, similar to AWAC\n",
    "    # small beta -> BC, big beta -> maximizing Q-value\n",
    "    beta: float = 10.0# 3.0\n",
    "    # coefficient for asymmetric critic loss\n",
    "    iql_tau: float = 0.9#0.7\n",
    "    # whether to use deterministic actor\n",
    "    iql_deterministic: bool = False\n",
    "    # total gradient updates during training\n",
    "    max_timesteps: int = int(1e6)\n",
    "    # maximum size of the replay buffer\n",
    "    buffer_size: int = 10_000_000\n",
    "    # training batch size\n",
    "    batch_size: int = 256\n",
    "    # whether to normalize states\n",
    "    normalize: bool = True\n",
    "    # whether to normalize reward (like in IQL)\n",
    "    normalize_reward: bool = True\n",
    "    # V-critic function learning rate\n",
    "    vf_lr: float = 3e-4\n",
    "    # Q-critic learning rate\n",
    "    qf_lr: float = 3e-4\n",
    "    # actor learning rate\n",
    "    actor_lr: float = 3e-4\n",
    "    #  where to use dropout for policy network, optional\n",
    "    actor_dropout: Optional[float] = None\n",
    "    # evaluation frequency, will evaluate every eval_freq training steps\n",
    "    eval_freq: int = int(5e3)\n",
    "    # number of episodes to run during evaluation\n",
    "    n_episodes: int = 100\n",
    "    # path for checkpoints saving, optional\n",
    "    checkpoints_path: Optional[str] = None\n",
    "    # file name for loading a model, optional\n",
    "    load_model: str = # \"/content/drive/MyDrive/IQL_report/checkpoint_194999.pt\"#\"checkpoint_194999.pt\"\n",
    "    # training random seed\n",
    "    seed: int = 0\n",
    "    # training device\n",
    "    device: str = \"cpu\"# cuda\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.name = f\"{self.name}-{self.env}-{str(uuid.uuid4())[:8]}\"\n",
    "        if self.checkpoints_path is not None:\n",
    "            self.checkpoints_path = os.path.join(self.checkpoints_path, self.name)\n",
    "\n",
    "\n",
    "def soft_update(target: nn.Module, source: nn.Module, tau: float):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_((1 - tau) * target_param.data + tau * source_param.data)\n",
    "\n",
    "\n",
    "def compute_mean_std(states: np.ndarray, eps: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mean = states.mean(0)\n",
    "    std = states.std(0) + eps\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def normalize_states(states: np.ndarray, mean: np.ndarray, std: np.ndarray):\n",
    "    return (states - mean) / std\n",
    "\n",
    "\n",
    "def wrap_env(\n",
    "    env: gym.Env,\n",
    "    state_mean: Union[np.ndarray, float] = 0.0,\n",
    "    state_std: Union[np.ndarray, float] = 1.0,\n",
    "    reward_scale: float = 1.0,\n",
    ") -> gym.Env:\n",
    "    # PEP 8: E731 do not assign a lambda expression, use a def\n",
    "    def normalize_state(state):\n",
    "        return (\n",
    "            state - state_mean\n",
    "        ) / state_std  # epsilon should be already added in std.\n",
    "\n",
    "    def scale_reward(reward):\n",
    "        # Please be careful, here reward is multiplied by scale!\n",
    "        return reward_scale * reward\n",
    "\n",
    "    env = gym.wrappers.TransformObservation(env, normalize_state)\n",
    "    if reward_scale != 1.0:\n",
    "        env = gym.wrappers.TransformReward(env, scale_reward)\n",
    "    return env\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        buffer_size: int,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self._buffer_size = buffer_size\n",
    "        self._pointer = 0\n",
    "        self._size = 0\n",
    "        print(device)\n",
    "        self._states = torch.zeros(\n",
    "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._actions = torch.zeros(\n",
    "            (buffer_size, action_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._next_states = torch.zeros(\n",
    "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._device = device\n",
    "\n",
    "    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
    "        return torch.tensor(data, dtype=torch.float32, device=self._device)\n",
    "\n",
    "    # Loads data in d4rl format, i.e. from Dict[str, np.array].\n",
    "    def load_d4rl_dataset(self, data: Dict[str, np.ndarray]):\n",
    "        if self._size != 0:\n",
    "            raise ValueError(\"Trying to load data into non-empty replay buffer\")\n",
    "        n_transitions = data[\"observations\"].shape[0]\n",
    "        if n_transitions > self._buffer_size:\n",
    "            raise ValueError(\n",
    "                \"Replay buffer is smaller than the dataset you are trying to load!\"\n",
    "            )\n",
    "        self._states[:n_transitions] = self._to_tensor(data[\"observations\"])\n",
    "        self._actions[:n_transitions] = self._to_tensor(data[\"actions\"])\n",
    "        self._rewards[:n_transitions] = self._to_tensor(data[\"rewards\"][..., None])\n",
    "        self._next_states[:n_transitions] = self._to_tensor(data[\"next_observations\"])\n",
    "        self._dones[:n_transitions] = self._to_tensor(data[\"terminals\"][..., None])\n",
    "        self._size += n_transitions\n",
    "        self._pointer = min(self._size, n_transitions)\n",
    "\n",
    "        print(f\"Dataset size: {n_transitions}\")\n",
    "\n",
    "    def sample(self, batch_size: int) -> TensorBatch:\n",
    "        indices = np.random.randint(0, min(self._size, self._pointer), size=batch_size)\n",
    "        states = self._states[indices]\n",
    "        actions = self._actions[indices]\n",
    "        rewards = self._rewards[indices]\n",
    "        next_states = self._next_states[indices]\n",
    "        dones = self._dones[indices]\n",
    "        return [states, actions, rewards, next_states, dones]\n",
    "\n",
    "    def add_transition(self):\n",
    "        # Use this method to add new data into the replay buffer during fine-tuning.\n",
    "        # I left it unimplemented since now we do not do fine-tuning.\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def set_seed(\n",
    "    seed: int, env: Optional[gym.Env] = None, deterministic_torch: bool = False\n",
    "):\n",
    "    if env is not None:\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(deterministic_torch)\n",
    "\n",
    "\n",
    "def wandb_init(config: dict) -> None:\n",
    "    wandb.init(\n",
    "        config=config,\n",
    "        project=config[\"project\"],\n",
    "        group=config[\"group\"],\n",
    "        name=config[\"name\"],\n",
    "        id=str(uuid.uuid4()),\n",
    "    )\n",
    "    wandb.run.save()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_actor(\n",
    "    env: gym.Env, actor: nn.Module, device: str, n_episodes: int, seed: int\n",
    ") -> np.ndarray:\n",
    "    env.seed(seed)\n",
    "    actor.eval()\n",
    "    episode_rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0.0\n",
    "        while not done:\n",
    "            action = actor.act(state, device)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    actor.train()\n",
    "    return np.asarray(episode_rewards)\n",
    "\n",
    "\n",
    "def return_reward_range(dataset, max_episode_steps):\n",
    "    returns, lengths = [], []\n",
    "    ep_ret, ep_len = 0.0, 0\n",
    "    for r, d in zip(dataset[\"rewards\"], dataset[\"terminals\"]):\n",
    "        ep_ret += float(r)\n",
    "        ep_len += 1\n",
    "        if d or ep_len == max_episode_steps:\n",
    "            returns.append(ep_ret)\n",
    "            lengths.append(ep_len)\n",
    "            ep_ret, ep_len = 0.0, 0\n",
    "    lengths.append(ep_len)  # but still keep track of number of steps\n",
    "    assert sum(lengths) == len(dataset[\"rewards\"])\n",
    "    return min(returns), max(returns)\n",
    "\n",
    "\n",
    "def modify_reward(dataset, env_name, max_episode_steps=1000):\n",
    "    if any(s in env_name for s in (\"halfcheetah\", \"hopper\", \"walker2d\")):\n",
    "        min_ret, max_ret = return_reward_range(dataset, max_episode_steps)\n",
    "        dataset[\"rewards\"] /= max_ret - min_ret\n",
    "        dataset[\"rewards\"] *= max_episode_steps\n",
    "    elif \"antmaze\" in env_name:\n",
    "        dataset[\"rewards\"] -= 1.0\n",
    "\n",
    "\n",
    "def asymmetric_l2_loss(u: torch.Tensor, tau: float) -> torch.Tensor:\n",
    "    return torch.mean(torch.abs(tau - (u < 0).float()) * u**2)\n",
    "\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.squeeze(dim=self.dim)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        activation_fn: Callable[[], nn.Module] = nn.ReLU,\n",
    "        output_activation_fn: Callable[[], nn.Module] = None,\n",
    "        squeeze_output: bool = False,\n",
    "        dropout: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        n_dims = len(dims)\n",
    "        if n_dims < 2:\n",
    "            raise ValueError(\"MLP requires at least two dims (input and output)\")\n",
    "\n",
    "        layers = []\n",
    "        for i in range(n_dims - 2):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(activation_fn())\n",
    "\n",
    "            if dropout is not None:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        if output_activation_fn is not None:\n",
    "            layers.append(output_activation_fn())\n",
    "        if squeeze_output:\n",
    "            if dims[-1] != 1:\n",
    "                raise ValueError(\"Last dim must be 1 when squeezing\")\n",
    "            layers.append(Squeeze(-1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        act_dim: int,\n",
    "        max_action: float,\n",
    "        hidden_dim: int = 256,\n",
    "        n_hidden: int = 2,\n",
    "        dropout: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = MLP(\n",
    "            [state_dim, *([hidden_dim] * n_hidden), act_dim],\n",
    "            output_activation_fn=nn.Tanh,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim, dtype=torch.float32))\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> Normal:\n",
    "        mean = self.net(obs)\n",
    "        std = torch.exp(self.log_std.clamp(LOG_STD_MIN, LOG_STD_MAX))\n",
    "        return Normal(mean, std)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray, device: str = \"cpu\"):\n",
    "        state = torch.tensor(state.reshape(1, -1), device=device, dtype=torch.float32)\n",
    "        dist = self(state)\n",
    "        action = dist.mean if not self.training else dist.sample()\n",
    "        action = torch.clamp(self.max_action * action, -self.max_action, self.max_action)\n",
    "        return action.cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        act_dim: int,\n",
    "        max_action: float,\n",
    "        hidden_dim: int = 256,\n",
    "        n_hidden: int = 2,\n",
    "        dropout: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = MLP(\n",
    "            [state_dim, *([hidden_dim] * n_hidden), act_dim],\n",
    "            output_activation_fn=nn.Tanh,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(obs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray, device: str = \"cpu\"):\n",
    "        state = torch.tensor(state.reshape(1, -1), device=device, dtype=torch.float32)\n",
    "        return (\n",
    "            torch.clamp(self(state) * self.max_action, -self.max_action, self.max_action)\n",
    "            .cpu()\n",
    "            .data.numpy()\n",
    "            .flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "class TwinQ(nn.Module):\n",
    "    def __init__(\n",
    "        self, state_dim: int, action_dim: int, hidden_dim: int = 256, n_hidden: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dims = [state_dim + action_dim, *([hidden_dim] * n_hidden), 1]\n",
    "        self.q1 = MLP(dims, squeeze_output=True)\n",
    "        self.q2 = MLP(dims, squeeze_output=True)\n",
    "\n",
    "    def both(\n",
    "        self, state: torch.Tensor, action: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        return self.q1(sa), self.q2(sa)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.min(*self.both(state, action))\n",
    "\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 256, n_hidden: int = 2):\n",
    "        super().__init__()\n",
    "        dims = [state_dim, *([hidden_dim] * n_hidden), 1]\n",
    "        self.v = MLP(dims, squeeze_output=True)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.v(state)\n",
    "\n",
    "\n",
    "class ImplicitQLearning:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_action: float,\n",
    "        actor: nn.Module,\n",
    "        actor_optimizer: torch.optim.Optimizer,\n",
    "        q_network: nn.Module,\n",
    "        q_optimizer: torch.optim.Optimizer,\n",
    "        v_network: nn.Module,\n",
    "        v_optimizer: torch.optim.Optimizer,\n",
    "        iql_tau: float = 0.7,\n",
    "        beta: float = 3.0,\n",
    "        max_steps: int = 1000000,\n",
    "        discount: float = 0.99,\n",
    "        tau: float = 0.005,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.max_action = max_action\n",
    "        self.qf = q_network\n",
    "        self.q_target = copy.deepcopy(self.qf).requires_grad_(False).to(device)\n",
    "        self.vf = v_network\n",
    "        self.actor = actor\n",
    "        self.v_optimizer = v_optimizer\n",
    "        self.q_optimizer = q_optimizer\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.actor_lr_schedule = CosineAnnealingLR(self.actor_optimizer, max_steps)\n",
    "        self.iql_tau = iql_tau\n",
    "        self.beta = beta\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "\n",
    "        self.total_it = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _update_v(self, observations, actions, log_dict) -> torch.Tensor:\n",
    "        # Update value function\n",
    "        with torch.no_grad():\n",
    "            target_q = self.q_target(observations, actions)\n",
    "\n",
    "        v = self.vf(observations)\n",
    "        adv = target_q - v\n",
    "        v_loss = asymmetric_l2_loss(adv, self.iql_tau)\n",
    "        log_dict[\"value_loss\"] = v_loss.item()\n",
    "        self.v_optimizer.zero_grad()\n",
    "        v_loss.backward()\n",
    "        self.v_optimizer.step()\n",
    "        return adv\n",
    "\n",
    "    def _update_q(\n",
    "        self,\n",
    "        next_v: torch.Tensor,\n",
    "        observations: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        terminals: torch.Tensor,\n",
    "        log_dict: Dict,\n",
    "    ):\n",
    "        targets = rewards + (1.0 - terminals.float()) * self.discount * next_v.detach()\n",
    "        qs = self.qf.both(observations, actions)\n",
    "        q_loss = sum(F.mse_loss(q, targets) for q in qs) / len(qs)\n",
    "        log_dict[\"q_loss\"] = q_loss.item()\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # Update target Q network\n",
    "        soft_update(self.q_target, self.qf, self.tau)\n",
    "\n",
    "    def _update_policy(\n",
    "        self,\n",
    "        adv: torch.Tensor,\n",
    "        observations: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        log_dict: Dict,\n",
    "    ):\n",
    "        exp_adv = torch.exp(self.beta * adv.detach()).clamp(max=EXP_ADV_MAX)\n",
    "        policy_out = self.actor(observations)\n",
    "        if isinstance(policy_out, torch.distributions.Distribution):\n",
    "            bc_losses = -policy_out.log_prob(actions).sum(-1, keepdim=False)\n",
    "        elif torch.is_tensor(policy_out):\n",
    "            if policy_out.shape != actions.shape:\n",
    "                raise RuntimeError(\"Actions shape missmatch\")\n",
    "            bc_losses = torch.sum((policy_out - actions) ** 2, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        policy_loss = torch.mean(exp_adv * bc_losses)\n",
    "        log_dict[\"actor_loss\"] = policy_loss.item()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.actor_lr_schedule.step()\n",
    "\n",
    "    def train(self, batch: TensorBatch) -> Dict[str, float]:\n",
    "        self.total_it += 1\n",
    "        (\n",
    "            observations,\n",
    "            actions,\n",
    "            rewards,\n",
    "            next_observations,\n",
    "            dones,\n",
    "        ) = batch\n",
    "        log_dict = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_v = self.vf(next_observations)\n",
    "        # Update value function\n",
    "        adv = self._update_v(observations, actions, log_dict)\n",
    "        rewards = rewards.squeeze(dim=-1)\n",
    "        dones = dones.squeeze(dim=-1)\n",
    "        # Update Q function\n",
    "        self._update_q(next_v, observations, actions, rewards, dones, log_dict)\n",
    "        # Update actor\n",
    "        self._update_policy(adv, observations, actions, log_dict)\n",
    "\n",
    "        return log_dict\n",
    "\n",
    "    def state_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"qf\": self.qf.state_dict(),\n",
    "            \"q_optimizer\": self.q_optimizer.state_dict(),\n",
    "            \"vf\": self.vf.state_dict(),\n",
    "            \"v_optimizer\": self.v_optimizer.state_dict(),\n",
    "            \"actor\": self.actor.state_dict(),\n",
    "            \"actor_optimizer\": self.actor_optimizer.state_dict(),\n",
    "            \"actor_lr_schedule\": self.actor_lr_schedule.state_dict(),\n",
    "            \"total_it\": self.total_it,\n",
    "        }\n",
    "    @staticmethod\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]):\n",
    "        self.qf.load_state_dict(state_dict[\"qf\"])\n",
    "        self.q_optimizer.load_state_dict(state_dict[\"q_optimizer\"])\n",
    "        self.q_target = copy.deepcopy(self.qf)\n",
    "\n",
    "        self.vf.load_state_dict(state_dict[\"vf\"])\n",
    "        self.v_optimizer.load_state_dict(state_dict[\"v_optimizer\"])\n",
    "\n",
    "        self.actor.load_state_dict(state_dict[\"actor\"])\n",
    "        self.actor_optimizer.load_state_dict(state_dict[\"actor_optimizer\"])\n",
    "        self.actor_lr_schedule.load_state_dict(state_dict[\"actor_lr_schedule\"])\n",
    "\n",
    "        self.total_it = state_dict[\"total_it\"]\n",
    "\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    env = gym.make(config.env)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    dataset = d4rl.qlearning_dataset(env)\n",
    "\n",
    "    if config.normalize_reward:\n",
    "        modify_reward(dataset, config.env)\n",
    "\n",
    "    if config.normalize:\n",
    "        state_mean, state_std = compute_mean_std(dataset[\"observations\"], eps=1e-3)\n",
    "    else:\n",
    "        state_mean, state_std = 0, 1\n",
    "\n",
    "    dataset[\"observations\"] = normalize_states(\n",
    "        dataset[\"observations\"], state_mean, state_std\n",
    "    )\n",
    "    dataset[\"next_observations\"] = normalize_states(\n",
    "        dataset[\"next_observations\"], state_mean, state_std\n",
    "    )\n",
    "    env = wrap_env(env, state_mean=state_mean, state_std=state_std)\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        config.buffer_size,\n",
    "        config.device,\n",
    "    )\n",
    "    replay_buffer.load_d4rl_dataset(dataset)\n",
    "    print(1)\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    if config.checkpoints_path is not None:\n",
    "        print(f\"Checkpoints path: {config.checkpoints_path}\")\n",
    "        os.makedirs(config.checkpoints_path, exist_ok=True)\n",
    "        with open(os.path.join(config.checkpoints_path, \"config.yaml\"), \"w\") as f:\n",
    "            pyrallis.dump(config, f)\n",
    "\n",
    "    # Set seeds\n",
    "    seed = config.seed\n",
    "    set_seed(seed, env)\n",
    "\n",
    "    q_network = TwinQ(state_dim, action_dim).to(config.device)\n",
    "    v_network = ValueFunction(state_dim).to(config.device)\n",
    "    actor = (\n",
    "        DeterministicPolicy(\n",
    "            state_dim, action_dim, max_action, dropout=config.actor_dropout\n",
    "        )\n",
    "        if config.iql_deterministic\n",
    "        else GaussianPolicy(\n",
    "            state_dim, action_dim, max_action, dropout=config.actor_dropout\n",
    "        )\n",
    "    ).to(config.device)\n",
    "    v_optimizer = torch.optim.Adam(v_network.parameters(), lr=config.vf_lr)\n",
    "    q_optimizer = torch.optim.Adam(q_network.parameters(), lr=config.qf_lr)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=config.actor_lr)\n",
    "    print(2)\n",
    "    kwargs = {\n",
    "        \"max_action\": max_action,\n",
    "        \"actor\": actor,\n",
    "        \"actor_optimizer\": actor_optimizer,\n",
    "        \"q_network\": q_network,\n",
    "        \"q_optimizer\": q_optimizer,\n",
    "        \"v_network\": v_network,\n",
    "        \"v_optimizer\": v_optimizer,\n",
    "        \"discount\": config.discount,\n",
    "        \"tau\": config.tau,\n",
    "        \"device\": config.device,\n",
    "        # IQL\n",
    "        \"beta\": config.beta,\n",
    "        \"iql_tau\": config.iql_tau,\n",
    "        \"max_steps\": config.max_timesteps,\n",
    "    }\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Training IQL, Env: {config.env}, Seed: {seed}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    # Initialize actor\n",
    "    trainer = ImplicitQLearning(**kwargs)\n",
    "    print(3)\n",
    "\n",
    "    if config.load_model != \"\":\n",
    "        policy_file = Path(config.load_model)\n",
    "        trainer.load_state_dict(torch.load(policy_file))\n",
    "        actor = trainer.actor\n",
    "        print(config.load_model)\n",
    "\n",
    "    wandb_init(asdict(config))\n",
    "    print(3)\n",
    "    evaluations = []\n",
    "    for t in range(int(config.max_timesteps)):\n",
    "        batch = replay_buffer.sample(config.batch_size)\n",
    "        batch = [b.to(config.device) for b in batch]\n",
    "        # log_dict = trainer.train(batch)\n",
    "        # wandb.log(log_dict, step=trainer.total_it)\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % config.eval_freq == 0:\n",
    "            print(f\"Time steps: {t + 1}\")\n",
    "            eval_scores = eval_actor(\n",
    "                env,\n",
    "                actor,\n",
    "                device=config.device,\n",
    "                n_episodes=config.n_episodes,\n",
    "                seed=config.seed,\n",
    "            )\n",
    "            eval_score = eval_scores.mean()\n",
    "            normalized_eval_score = env.get_normalized_score(eval_score) * 100.0\n",
    "            evaluations.append(normalized_eval_score)\n",
    "            print(\"---------------------------------------\")\n",
    "            print(\n",
    "                f\"Evaluation over {config.n_episodes} episodes: \"\n",
    "                f\"{eval_score:.3f} , D4RL score: {normalized_eval_score:.3f}\"\n",
    "            )\n",
    "            print(\"---------------------------------------\")\n",
    "            if config.checkpoints_path is not None:\n",
    "                torch.save(\n",
    "                    trainer.state_dict(),\n",
    "                    os.path.join(config.checkpoints_path, f\"checkpoint_{t}.pt\"),\n",
    "                )\n",
    "            wandb.log(\n",
    "                {\"d4rl_normalized_score\": normalized_eval_score}, step=trainer.total_it\n",
    "            )\n",
    "            print(4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = TrainConfig()\n",
    "    # config.checkpoints_path=\"MyDrive/IQL_report\"\n",
    "    # train(config)\n"
   ],
   "metadata": {
    "id": "V-W_xDMUg7EU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "step 8. Evoluation of trained model/agent"
   ],
   "metadata": {
    "id": "ypiVTycTTLY3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# source: https://github.com/gwthomas/IQL-PyTorch\n",
    "# https://arxiv.org/pdf/2110.06169.pdf\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import d4rl\n",
    "import gym\n",
    "import numpy as np\n",
    "import pyrallis\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.distributions import Normal\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "TensorBatch = List[torch.Tensor]\n",
    "\n",
    "\n",
    "EXP_ADV_MAX = 100.0\n",
    "LOG_STD_MIN = -20.0\n",
    "LOG_STD_MAX = 2.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # wandb project name\n",
    "    project: str = \"CORL\"\n",
    "    # wandb group name\n",
    "    group: str = \"IQL-D4RL\"\n",
    "    # wandb run name\n",
    "    name: str = \"IQL\"\n",
    "    # training dataset and evaluation environment\n",
    "    env: str = \"antmaze-medium-play\"#\"antmaze-medium-diverse\"#\"halfcheetah-medium-expert-v2\"\n",
    "    # discount factor\n",
    "    discount: float = 0.99\n",
    "    # coefficient for the target critic Polyak's update\n",
    "    tau: float = 0.005\n",
    "    # actor update inverse temperature, similar to AWAC\n",
    "    # small beta -> BC, big beta -> maximizing Q-value\n",
    "    beta: float = 10.0# 3.0\n",
    "    # coefficient for asymmetric critic loss\n",
    "    iql_tau: float = 0.9#0.7\n",
    "    # whether to use deterministic actor\n",
    "    iql_deterministic: bool = False\n",
    "    # total gradient updates during training\n",
    "    max_timesteps: int = int(1e6)\n",
    "    # maximum size of the replay buffer\n",
    "    buffer_size: int = 10_000_000\n",
    "    # training batch size\n",
    "    batch_size: int = 256\n",
    "    # whether to normalize states\n",
    "    normalize: bool = True\n",
    "    # whether to normalize reward (like in IQL)\n",
    "    normalize_reward: bool = True\n",
    "    # V-critic function learning rate\n",
    "    vf_lr: float = 3e-4\n",
    "    # Q-critic learning rate\n",
    "    qf_lr: float = 3e-4\n",
    "    # actor learning rate\n",
    "    actor_lr: float = 3e-4\n",
    "    #  where to use dropout for policy network, optional\n",
    "    actor_dropout: Optional[float] = None\n",
    "    # evaluation frequency, will evaluate every eval_freq training steps\n",
    "    eval_freq: int = int(5e3)\n",
    "    # number of episodes to run during evaluation\n",
    "    n_episodes: int = 100\n",
    "    # path for checkpoints saving, optional\n",
    "    checkpoints_path: Optional[str] = None\n",
    "    # file name for loading a model, optional\n",
    "    load_model: str = \"/content/drive/MyDrive/IQL_report/checkpoint_194999.pt\"\n",
    "    # training random seed\n",
    "    seed: int = 0\n",
    "    # training device\n",
    "    device: str = \"cpu\"# cuda\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.name = f\"{self.name}-{self.env}-{str(uuid.uuid4())[:8]}\"\n",
    "        if self.checkpoints_path is not None:\n",
    "            self.checkpoints_path = os.path.join(self.checkpoints_path, self.name)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_actor(\n",
    "    env: gym.Env, actor: nn.Module, device: str, n_episodes: int, seed: int\n",
    ") -> np.ndarray:\n",
    "    env.seed(seed)\n",
    "    actor.eval()\n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state, done = env.reset(), False\n",
    "        i = 0\n",
    "\n",
    "        episode_reward = 0.0\n",
    "        while not done:\n",
    "            action = actor.act(state, device)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # env.render()\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "              print(f\"step {i} done {done}episode_reward {episode_reward}\")\n",
    "              print(action)\n",
    "\n",
    "            i+=1\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    actor.train()\n",
    "    return np.asarray(episode_rewards)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = TrainConfig()\n",
    "\n",
    "    env = gym.make(config.env)\n",
    "    attrs = vars(env)\n",
    "    # {'kids': 0, 'name': 'Dog', 'color': 'Spotted', 'age': 10, 'legs': 2, 'smell': 'Alot'}\n",
    "    # now dump this in some way or another\n",
    "    print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    dataset = d4rl.qlearning_dataset(env)\n",
    "    print(dataset.keys())\n",
    "    print(dataset[\"terminals\"].shape)\n",
    "    print(dataset[\"terminals\"][0])\n",
    "    print(dataset[\"observations\"].shape)\n",
    "    print(dataset[\"observations\"][0])\n",
    "    print(dataset[\"actions\"].shape)\n",
    "    print(dataset[\"actions\"][0])\n",
    "\n",
    "    if config.normalize_reward:\n",
    "        modify_reward(dataset, config.env)\n",
    "\n",
    "    if config.normalize:\n",
    "        state_mean, state_std = compute_mean_std(dataset[\"observations\"], eps=1e-3)\n",
    "    else:\n",
    "        state_mean, state_std = 0, 1\n",
    "\n",
    "    dataset[\"observations\"] = normalize_states(\n",
    "        dataset[\"observations\"], state_mean, state_std\n",
    "    )\n",
    "    dataset[\"next_observations\"] = normalize_states(\n",
    "        dataset[\"next_observations\"], state_mean, state_std\n",
    "    )\n",
    "    env = wrap_env(env, state_mean=state_mean, state_std=state_std)\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        config.buffer_size,\n",
    "        config.device,\n",
    "    )\n",
    "    replay_buffer.load_d4rl_dataset(dataset)\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    if config.checkpoints_path is not None:\n",
    "        print(f\"Checkpoints path: {config.checkpoints_path}\")\n",
    "        os.makedirs(config.checkpoints_path, exist_ok=True)\n",
    "        with open(os.path.join(config.checkpoints_path, \"config.yaml\"), \"w\") as f:\n",
    "            pyrallis.dump(config, f)\n",
    "\n",
    "    # Set seeds\n",
    "    seed = config.seed\n",
    "    set_seed(seed, env)\n",
    "\n",
    "    q_network = TwinQ(state_dim, action_dim).to(config.device)\n",
    "    v_network = ValueFunction(state_dim).to(config.device)\n",
    "    actor = (\n",
    "        DeterministicPolicy(\n",
    "            state_dim, action_dim, max_action, dropout=config.actor_dropout\n",
    "        )\n",
    "        if config.iql_deterministic\n",
    "        else GaussianPolicy(\n",
    "            state_dim, action_dim, max_action, dropout=config.actor_dropout\n",
    "        )\n",
    "    ).to(config.device)\n",
    "    v_optimizer = torch.optim.Adam(v_network.parameters(), lr=config.vf_lr)\n",
    "    q_optimizer = torch.optim.Adam(q_network.parameters(), lr=config.qf_lr)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=config.actor_lr)\n",
    "    kwargs = {\n",
    "        \"max_action\": max_action,\n",
    "        \"actor\": actor,\n",
    "        \"actor_optimizer\": actor_optimizer,\n",
    "        \"q_network\": q_network,\n",
    "        \"q_optimizer\": q_optimizer,\n",
    "        \"v_network\": v_network,\n",
    "        \"v_optimizer\": v_optimizer,\n",
    "        \"discount\": config.discount,\n",
    "        \"tau\": config.tau,\n",
    "        \"device\": config.device,\n",
    "        # IQL\n",
    "        \"beta\": config.beta,\n",
    "        \"iql_tau\": config.iql_tau,\n",
    "        \"max_steps\": config.max_timesteps,\n",
    "    }\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Training IQL, Env: {config.env}, Seed: {seed}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    # Initialize actor\n",
    "    trainer = ImplicitQLearning(**kwargs)\n",
    "    print(3)\n",
    "\n",
    "    if config.load_model != \"\":\n",
    "        policy_file = Path(config.load_model)\n",
    "        trainer.load_state_dict(torch.load(policy_file))\n",
    "        actor = trainer.actor\n",
    "        print(config.load_model)\n",
    "\n",
    "    eval_scores = eval_actor(\n",
    "                env,\n",
    "                actor,\n",
    "                device=config.device,\n",
    "                n_episodes=config.n_episodes,\n",
    "                seed=config.seed,\n",
    "            )\n",
    "    eval_score = eval_scores.mean()\n",
    "    normalized_eval_score = env.get_normalized_score(eval_score) * 100.0\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\n",
    "        f\"Evaluation over {config.n_episodes} episodes: \"\n",
    "        f\"{eval_score:.3f} , D4RL score: {normalized_eval_score:.3f}\"\n",
    "    )\n"
   ],
   "metadata": {
    "id": "auTrORGU21VF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "d2d6cb29-ef43-43fe-b989-d312472354c4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:511: UserWarning: \u001B[33mWARN: Using the latest versioned environment `antmaze-medium-play-v2` instead of the unversioned environment `antmaze-medium-play`\u001B[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:84: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Goal:  (20.451066965914244, 21.401942184042255)\n",
      "env: <OrderEnforcingNormalized: <AntMazeEnv instance>>, _action_space: None, _observation_space: None, _reward_range: None, _metadata: None, _max_episode_steps: 1000, _elapsed_steps: None\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['observations', 'actions', 'next_observations', 'rewards', 'terminals'])\n",
      "(999000,)\n",
      "False\n",
      "(999000, 29)\n",
      "[ 2.0075781e+01  4.8538476e-01  7.1943611e-01  9.9470818e-01\n",
      " -1.9516461e-03  2.2971349e-02 -1.0012076e-01  9.1006132e-03\n",
      " -9.7871117e-02  4.7545791e-02 -9.1872640e-02 -3.9479494e-02\n",
      "  4.7046404e-02 -8.7086745e-02 -6.2923282e-02 -1.9017500e-01\n",
      " -6.2165249e-02 -7.2244927e-02  1.5443078e-01 -1.4765161e-01\n",
      " -7.6437020e-03  8.1294313e-02  4.7078412e-02  6.2306028e-02\n",
      "  6.9461376e-03  6.1173752e-02  9.7111790e-03  3.5539951e-02\n",
      " -1.8500645e-01]\n",
      "(999000, 8)\n",
      "[ 0.87698334 -0.6472296   0.9834756  -1.         -0.5234662   1.\n",
      " -0.5868562   0.81721026]\n",
      "cpu\n",
      "Dataset size: 999000\n",
      "---------------------------------------\n",
      "Training IQL, Env: antmaze-medium-play, Seed: 0\n",
      "---------------------------------------\n",
      "3\n",
      "/content/drive/MyDrive/IQL_report/checkpoint_194999.pt\n",
      "Target Goal:  (20.850195191963145, 20.98763095787087)\n",
      "step 364 done Trueepisode_reward 1.0\n",
      "[ 0.03471806 -0.89470834  0.10668194 -0.6572366  -0.34113866  0.77764446\n",
      "  0.20691787 -0.04012622]\n",
      "Target Goal:  (20.642448404970253, 21.091780613457697)\n",
      "step 999 done Trueepisode_reward 0.0\n",
      "[-0.53766316  0.9124525  -0.6388453  -0.2394695  -0.01336895  0.7784319\n",
      " -0.7692112   0.9222782 ]\n",
      "Target Goal:  (21.359525279542364, 20.64788897870223)\n",
      "step 489 done Trueepisode_reward 1.0\n",
      "[-0.39667836 -0.9578291  -0.8909944  -0.8389999   0.1919121   0.9773871\n",
      "  0.84514624  0.04009106]\n",
      "Target Goal:  (20.603562590192876, 20.96916128814343)\n",
      "step 348 done Trueepisode_reward 1.0\n",
      "[-0.10558254 -0.64749634 -0.99474174 -0.93362325  0.99597234  0.9468908\n",
      "  0.9537263  -0.86701554]\n",
      "Target Goal:  (20.409296772915248, 21.26762591967135)\n",
      "step 999 done Trueepisode_reward 0.0\n",
      "[ 0.5323507  -0.57662946 -0.73626864 -0.9675836  -0.12569456 -0.6119229\n",
      " -0.7100104   0.79223126]\n",
      "Target Goal:  (21.20935802335923, 21.189423152359954)\n",
      "step 999 done Trueepisode_reward 0.0\n",
      "[ 0.6323772  -0.54152983 -0.7838755  -0.01996462  0.9582646   0.34287593\n",
      " -0.8882644   0.5326936 ]\n",
      "Target Goal:  (20.189951069573457, 21.112255479852315)\n",
      "step 301 done Trueepisode_reward 1.0\n",
      "[-0.61353874 -0.61101305 -0.95969605 -0.9463361   0.87873745  0.946386\n",
      "  0.9303217  -0.6046017 ]\n",
      "Target Goal:  (20.654126127802385, 20.801778784707633)\n",
      "step 324 done Trueepisode_reward 1.0\n",
      "[-0.40038317 -0.7239075   0.17920673 -0.937824   -0.18484898  0.48887527\n",
      " -0.64962506  0.6945677 ]\n",
      "Target Goal:  (20.465545232434724, 20.877251697406585)\n",
      "step 335 done Trueepisode_reward 1.0\n",
      "[-0.09005506  0.56471515 -0.39540413 -0.64554846 -0.08424642  0.9022796\n",
      "  0.86651534  0.6000575 ]\n",
      "Target Goal:  (21.083969761979734, 20.9578441464265)\n",
      "step 424 done Trueepisode_reward 1.0\n",
      "[-0.08380154  0.96955866 -0.6244868  -0.9285978   0.8665491   0.9006358\n",
      " -0.58953977  0.7707988 ]\n",
      "Target Goal:  (20.708323498537418, 20.467144689613974)\n",
      "step 999 done Trueepisode_reward 0.0\n",
      "[-0.95349896  0.06781609 -0.83135045  0.07661634  0.72190285  0.98675424\n",
      "  0.7887936  -0.03983349]\n",
      "Target Goal:  (20.77195799598259, 20.735101018445583)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-17114f2b4af1>\u001B[0m in \u001B[0;36m<cell line: 115>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    213\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 215\u001B[0;31m     eval_scores = eval_actor(\n\u001B[0m\u001B[1;32m    216\u001B[0m                 \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    217\u001B[0m                 \u001B[0mactor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mctx_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 115\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-33-17114f2b4af1>\u001B[0m in \u001B[0;36meval_actor\u001B[0;34m(env, actor, device, n_episodes, seed)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m             \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mactor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m             \u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m             \u001B[0;31m# env.render()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m             \u001B[0mepisode_reward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 314\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    315\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_max_episode_steps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_has_reset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Cannot call env.step() before calling reset()\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/locomotion/wrappers.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    159\u001B[0m         \u001B[0mscaled_action\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscaled_action\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mub\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m         \u001B[0mwrapped_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrapped_env\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscaled_action\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0mnext_obs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwrapped_step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_normalize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/locomotion/maze_env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mold_pos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    301\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 302\u001B[0;31m       \u001B[0minner_next_obs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minner_reward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLOCOMOTION_ENV\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    303\u001B[0m     \u001B[0mnext_obs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_obs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mnext_obs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minner_reward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/locomotion/goal_reaching_env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, a)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBASE_ENV\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreward_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'dense'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m       \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_goal\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_xy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/d4rl/locomotion/ant.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, a)\u001B[0m\n\u001B[1;32m     69\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[0mxposbefore\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_body_com\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"torso\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_simulation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe_skip\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     72\u001B[0m     \u001B[0mxposafter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_body_com\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"torso\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m     \u001B[0mforward_reward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mxposafter\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mxposbefore\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py\u001B[0m in \u001B[0;36mdo_simulation\u001B[0;34m(self, ctrl, n_frames)\u001B[0m\n\u001B[1;32m    138\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctrl\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mctrl\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_frames\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m     def render(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "IQL agent is able to achieve total average 70 out of 100 goals."
   ],
   "metadata": {
    "id": "McHWTez7TSwQ"
   }
  }
 ]
}